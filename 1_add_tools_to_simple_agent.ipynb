{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents: Adding tools to an agent\n",
    "---\n",
    "\n",
    "`Tools` are an abstraction in LangChain that associates a python function with a schema that defines the function's name, description and expected arguments. `Tools` can then be passed to chat models that support `tool calling` allowing the model to request the execution of that function with the required parameters as provided in the schema.\n",
    "\n",
    "#### What is Tool Calling?\n",
    "\n",
    "Many AI applications interact directly with humans but what if there was a way for Large Language Models (`LLMs`) to interact directly with systems, such as your databases or external APIs? These systems often have a specific input schema, for example APIs frequently require a payload structure. For this, tool calling enables this functionality:\n",
    "\n",
    "1. **Tool Creation**: Use the `@tool` decorator to create a tool. A tool is an association between a function and its respective schema.\n",
    "\n",
    "1. **Tool Binding**: The tool needs to be connected to a model that supports tool calling. This gives model the awareness of the tool and the associated input schema that is required.\n",
    "\n",
    "1. **Tool Calling**: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's output schema.\n",
    "\n",
    "2. **Tool Execution**: The tool can be executed using the arguments provided by the model.\n",
    "\n",
    "In this short lab, we will see how we can create tools, use tools and use that within our simple agent. We will use the same example from our previous notebook to create a simple agent with certain tools. \n",
    "\n",
    "![tool-calling](img/tool_calling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets import the necessary libraries required to build the agent in this notebook\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Agent State\n",
    "\n",
    "We will first define the agent state that will remain throughout the operation. This is the state of the graph and the state schema serves as the input for all nodes and edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], \"The messages in the conversation\"]\n",
    "    itinerary: Optional[str]\n",
    "    city: str\n",
    "    user_message: str\n",
    "    weather_info: Optional[str]\n",
    "    attractions_info: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up language models and prompts\n",
    "\n",
    "Next, we will set up LLM and a prompt template that will be used by our agent in planning the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the global variables used across this notebook\n",
    "BEDROCK_RUNTIME: str = 'bedrock-runtime'\n",
    "# Model ID used by the agent\n",
    "AMAZON_NOVA_LITE_MODEL_ID: str = \"us.amazon.nova-lite-v1:0\"\n",
    "PROVIDER_ID : str = 'amazon'\n",
    "# Inference parameters\n",
    "TEMPERATURE: float = 0.1\n",
    "MAX_TOKENS: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "# We are importing this to use any model supported on Amazon Bedrock. In this example\n",
    "# we will be using the Amazon Nova lite model.\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "# This helps checkpoint the memory state of the agent for short term/long term memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "logger.info(f\"Running this example in region: {region}\")\n",
    "\n",
    "# Initialize the bedrock client placeholder\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm used by the agent\n",
    "llm = ChatBedrockConverse(\n",
    "    model=AMAZON_NOVA_LITE_MODEL_ID,\n",
    "    provider=PROVIDER_ID, \n",
    "    temperature=TEMPERATURE, \n",
    "    max_tokens=MAX_TOKENS,\n",
    "    client=bedrock_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tools and binding them to an LLM\n",
    "---\n",
    "\n",
    "In this section of the notebook, we have now created the `llm` (using `Amazon Nova Lite`). `Amazon Nova Lite` supports function calling. In this case, we can create a function or tools with the `@tool` decorator and associate these tools to the `llm`. The `llm` will then use the tools during invocation and determine which tool to use when a user asks a question.\n",
    "\n",
    "\n",
    "```python\n",
    "# Tool creation\n",
    "tools = [my_tool]\n",
    "# Tool binding\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "# Tool calling \n",
    "response = model_with_tools.invoke(user_input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create two tools that our `LLM` will have access to while executing the user request:\n",
    "\n",
    "1. **Search tourist attractions**: We will create a function that does a web search using the `TAVILY` API. This API will be used with the city as requested by the user to look for tourist attractions.\n",
    "\n",
    "1. **Get weather forecast**: This tool will be used to get insights into the current weather in the city as requested by the user for which they are wanting to create an itinerary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an ideal scenaio, the tools would call some APIs, such as tavily for web search and the weather API for weather information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import json\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def mock_search_tourist_attractions(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for tourist attractions in the specified city using a local JSON file.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to search for attractions\n",
    "        \n",
    "    Returns:\n",
    "        A string containing information about tourist attractions in the city\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Path to the JSON file containing the attraction data\n",
    "        file_path = os.path.join(\"data\", \"attractions.json\")\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: Attractions data file not found. Please make sure '{file_path}' exists.\"\n",
    "        \n",
    "        # Load the attractions data from the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            attractions_data = json.load(f)\n",
    "        \n",
    "        # Check if the city exists in the data\n",
    "        if city.lower() not in attractions_data:\n",
    "            return f\"No information available for tourist attractions in {city}. Try another city.\"\n",
    "        \n",
    "        # Get the attractions for the specified city\n",
    "        city_attractions = attractions_data[city.lower()]\n",
    "        \n",
    "        # Format the attractions information\n",
    "        attractions = f\"Top boating and swimming attractions in {city}:\\n\"\n",
    "        for i, attraction in enumerate(city_attractions, 1):\n",
    "            attractions += f\"{i}. {attraction['title']}: {attraction['description'][:150]}...\\n\"\n",
    "        \n",
    "        return attractions\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while searching for tourist attractions in {city}: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def mock_get_weather_forecast(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather forecast for the specified city using a local JSON file.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get the weather forecast for\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the weather forecast for the city\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Path to the JSON file containing the weather data\n",
    "        file_path = os.path.join(\"data\", \"weather.json\")\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: Weather data file not found. Please make sure '{file_path}' exists.\"\n",
    "        \n",
    "        # Load the weather data from the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            weather_data = json.load(f)\n",
    "        \n",
    "        # Check if the city exists in the data\n",
    "        if city.lower() not in weather_data:\n",
    "            return f\"No weather information available for {city}. Try another city.\"\n",
    "        \n",
    "        # Get the weather data for the specified city\n",
    "        city_weather = weather_data[city.lower()]\n",
    "        \n",
    "        # Extract location and current weather information\n",
    "        location = city_weather[\"location\"]\n",
    "        current = city_weather[\"current\"]\n",
    "        \n",
    "        # Format the weather information\n",
    "        forecast = f\"Current weather in {location['name']}, {location['country']}:\\n\"\n",
    "        forecast += f\"Local time: {location['localtime']}\\n\"\n",
    "        forecast += f\"Temperature: {current['temperature']}°C\\n\"\n",
    "        forecast += f\"Weather: {', '.join(current['weather_descriptions'])}\\n\"\n",
    "        forecast += f\"Feels like: {current['feelslike']}°C\\n\"\n",
    "        forecast += f\"Humidity: {current['humidity']}%\\n\"\n",
    "        forecast += f\"Wind: {current['wind_speed']} km/h, {current['wind_dir']}\\n\"\n",
    "        forecast += f\"Pressure: {current['pressure']} mb\\n\"\n",
    "        forecast += f\"Visibility: {current['visibility']} km\\n\"\n",
    "        forecast += f\"UV Index: {current['uv_index']}\\n\"\n",
    "        \n",
    "        # Add precipitation information if available\n",
    "        if 'precip' in current:\n",
    "            forecast += f\"Precipitation: {current['precip']} mm\\n\"\n",
    "            \n",
    "        # Add cloud cover information if available\n",
    "        if 'cloudcover' in current:\n",
    "            forecast += f\"Cloud cover: {current['cloudcover']}%\\n\"\n",
    "            \n",
    "        # Add air quality information if available\n",
    "        if 'air_quality' in current:\n",
    "            aq = current['air_quality']\n",
    "            forecast += \"\\nAir Quality:\\n\"\n",
    "            forecast += f\"US EPA Index: {aq['us-epa-index']} \"\n",
    "            \n",
    "            # Add interpretation of EPA index\n",
    "            epa_index = aq['us-epa-index']\n",
    "            if epa_index == 1:\n",
    "                forecast += \"(Good)\\n\"\n",
    "            elif epa_index == 2:\n",
    "                forecast += \"(Moderate)\\n\"\n",
    "            elif epa_index == 3:\n",
    "                forecast += \"(Unhealthy for sensitive groups)\\n\"\n",
    "            elif epa_index == 4:\n",
    "                forecast += \"(Unhealthy)\\n\"\n",
    "            elif epa_index == 5:\n",
    "                forecast += \"(Very Unhealthy)\\n\"\n",
    "            elif epa_index == 6:\n",
    "                forecast += \"(Hazardous)\\n\"\n",
    "            else:\n",
    "                forecast += \"\\n\"\n",
    "                \n",
    "        return forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while getting the weather forecast for {city}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [mock_search_tourist_attractions, mock_get_weather_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create the ReAct agent with the correct prompt\n",
    "get_realtime_info_react_llm = create_react_agent(\n",
    "    llm,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_realtime_info_react_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the nodes and Edges\n",
    "---\n",
    "\n",
    "Next, we will be adding nodes, edges and persistent memory to the `StateGraph` before we compile it.\n",
    "\n",
    "1. user travel plans\n",
    "1. invoke with Bedrock\n",
    "1. generate the travel plan for the day\n",
    "1. ability to add or modify the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first node that solely inputs a user message\n",
    "# The first node that takes in user input\n",
    "def input_interest(state: PlannerState) -> PlannerState:\n",
    "    \"\"\"\n",
    "    This function processes the user's message.\n",
    "    \"\"\"\n",
    "    # Initialize messages if needed\n",
    "    if not state.get('messages'): \n",
    "        state['messages'] = []\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        **state\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState) -> PlannerState:\n",
    "    try:\n",
    "        messages = [\n",
    "            HumanMessage(content=state['user_message'])\n",
    "        ]\n",
    "        agent_input = {\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        \n",
    "        # Invoke the agent\n",
    "        result = get_realtime_info_react_llm.invoke(agent_input)\n",
    "        \n",
    "        # Extract the output\n",
    "        if isinstance(result, dict) and \"output\" in result:\n",
    "            itinerary = result[\"output\"]\n",
    "        else:\n",
    "            itinerary = str(result)\n",
    "        \n",
    "        # Return state\n",
    "        return {\n",
    "            **state,\n",
    "            'messages': [\n",
    "                *state.get('messages', []),\n",
    "                HumanMessage(content=state['user_message']),\n",
    "                AIMessage(content=itinerary)\n",
    "            ],\n",
    "            'itinerary': itinerary\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating itinerary: {e}\")\n",
    "        error_message = f\"I apologize, but I encountered an error while creating your itinerary: {str(e)}\"\n",
    "        \n",
    "        # Print the exception for debugging\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Return updated state with error\n",
    "        return {\n",
    "            **state,\n",
    "            'messages': [\n",
    "                *state['messages'],\n",
    "                HumanMessage(content=state['user_message']),\n",
    "                AIMessage(content=error_message)\n",
    "            ],\n",
    "            'itinerary': error_message\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Graph\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we will create our `LangGraph` workflow and then compile it.\n",
    "\n",
    "1. First, we will initialize the `StateGraph` with the `State` class that we defined above\n",
    "1. Then, we add our nodes and edges.\n",
    "1. We use the `START` Node, a special node that sends user input to the graph, to indicate where to start our graph.\n",
    "1. The `END` Node is a special node that represents a terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the StateGraph\n",
    "workflow = StateGraph(PlannerState)\n",
    "# Next, we will add our nodes to this workflowa\n",
    "workflow.add_node(\"input_user_interests\", input_interest)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "workflow.set_entry_point(\"input_user_interests\")\n",
    "# Next, we will add a direct edge between input interests and create itinerary\n",
    "workflow.add_edge(\"input_user_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create a checkpointer which will let us have the graph persist its state\n",
    "# this is a complete memory for the entire graph\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have compiled the graph, we can view it in a mermaid diagram. You can use Mermaid\n",
    "# to generate diagrams from markdown-like text.\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that from the start point, the user can input the message or their interests for the itinerary. Next, according to the next node, the llm will be invoked with the custom prompt template to create that itinerary. This is a hello world example of a simple agent on `LangGraph`.\n",
    "\n",
    "### Define the function that runs the graph\n",
    "---\n",
    "\n",
    "When we compile the graph, we turn it into a `LangChain` Runnable, which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs. In the following example, we run `stream()` to invoke the graph with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(result):\n",
    "    \"\"\"\n",
    "    Pretty prints the raw response from a LangGraph workflow result.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    \n",
    "    # For dictionary results\n",
    "    if isinstance(result, dict):\n",
    "        if \"messages\" in result:\n",
    "            # Print each message in a readable format\n",
    "            for i, message in enumerate(result[\"messages\"]):\n",
    "                # Get message type\n",
    "                msg_type = type(message).__name__\n",
    "                print(f\"[Message {i+1}] Type: {msg_type}\")\n",
    "                \n",
    "                # Print content in readable format\n",
    "                if hasattr(message, \"content\"):\n",
    "                    print(\"\\nContent:\")\n",
    "                    if isinstance(message.content, list):\n",
    "                        for j, part in enumerate(message.content):\n",
    "                            print(f\"\\n-- Part {j+1} --\")\n",
    "                            pprint(part)\n",
    "                    else:\n",
    "                        print(message.content)\n",
    "                    \n",
    "                print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "        else:\n",
    "            # Just pretty print the dictionary\n",
    "            pprint(result)\n",
    "    \n",
    "    # For list results\n",
    "    elif isinstance(result, list):\n",
    "        for i, item in enumerate(result):\n",
    "            print(f\"\\n[Item {i+1}]\\n\")\n",
    "            print_stream(item)  # Recursively handle items\n",
    "    \n",
    "    # For other types\n",
    "    else:\n",
    "        print(f\"Type: {type(result)}\")\n",
    "        print(\"\\nContent:\")\n",
    "        print(result)\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "input_data = {\"user_message\": \"I'm interested in visiting Paris for 3 days and I love art, history, and food. Can you suggest an itinerary?\"}\n",
    "\n",
    "# Run the workflow with your input\n",
    "result = app.invoke(input_data, config=config)\n",
    "\n",
    "# Display the result\n",
    "print_stream(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
