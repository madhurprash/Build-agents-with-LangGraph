{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents: Some key concepts\n",
    "---\n",
    "\n",
    "### What is an Agent?\n",
    "\n",
    "In context of Large Language Models (`LLMs`), an agent is an autonomous or semi-autonomous system that can take actions on behalf of the user in a given environment, state, and make decisions or take actions that can accomplish certain tasks. A more detailed explanation of agents would be systems powered by `LLMs`, or `LLMs` themselves that have access to a certain user environment, external sources (such as data) that they can use to accomplish user tasks by efficiently breaking the request into smaller components. An agent can perform these tasks using different components given below:\n",
    "\n",
    "### Tools\n",
    "Tools are simple functions (can be python functions) that agents use to interact with `APIs` or external data sources. Tools can be of various kinds but can be divided into two portions:\n",
    "\n",
    "- `Built-in tools`: Several Agentic frameworks that help you build agents provide tools that are already implemented that you can use through your agent such as:\n",
    "    1. Web search tools to retrieve information from the web (such as `DuckDuckGo`, `Bing Search`, `Wikipedia API`).\n",
    "    1. Database tools such as `SQL Database connector` to execute SQL queries against relational databases.\n",
    "    1. Different frameworks provide a host of different built-in tools. For example, view some built-in tools offered by `LangChain` [here](https://python.langchain.com/docs/concepts/tools/).\n",
    "\n",
    "- `Custom tools`: You can build your own custom tools that perform various actions by interacting with your choice of custom/proprietary `APIs`. View some of these examples below:\n",
    "    1. **Document parsing**: Extract and analyze text from PDF documents, create summaries of long documents, etc. \n",
    "    1. **Task management**: Create and assign tasks in project management systems for your specific use cases and customer stories.\n",
    "    1. **Budget analyzer**: Categorize and analyze spending patterns across your company infrastructure using custom `APIs`.\n",
    "    1. **Code analyzer**: Review code for bugs or improvements using your own fine-tuned model.\n",
    "\n",
    "**Note**: Writing tools are simple as writing functions. You can write tools using different approaches via different frameworks. In this lab, we will go over how we can write tools in `LangGraph`.\n",
    "\n",
    "### Workflows\n",
    "Workflows are how agents can use a predefined sequence of operations that define how an agent can process information and take action. For example, a single agent can have access to multiple tools and be prompted to perform those actions either in a \n",
    "\n",
    "1. `sequential` manner (executing tools one by one), \n",
    "1. `branching`, where the agent dynamically determines which tool to call based on the step it took previously, \n",
    "1. `Cyclic`, where the agent can perform the same set of steps with a human in the loop to review and iterate on the new results or -\n",
    "1. It can be a combination of various workflows and agents interacting all together.\n",
    "\n",
    "### Persistence/Memory\n",
    "Imagine if you had an agent powering your chatbot serving thousands of users. In that case, it becomes important to retain the memory and the state of the agent with specific user interactions. This is essential for:\n",
    "1. Maintaining conversation context, \n",
    "1. Resuming interrupted operations\n",
    "1. Tracking process across multiple user sessions.\n",
    "Using `LangGraph`, you can retain `short term memory` (which refers to a thread-scoped memory, can be recalled at any time from within a single conversational thread with a user) or `long term memory` (which refers to memory that is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID.)\n",
    "\n",
    "### Human-in-the-loop (`HITL`)\n",
    "Agents act autonomously and need to be evaluated. This requires iterative testing and a human, who can check the steps that an agent takes and makes sure that the agent is performing and executing the right steps and tools. For this, it is important to have a human-in-the-loop. \n",
    "- This workflow integrates a human input into automated processes, allowing for decisions.\n",
    "- Useful in LLM-based applications to check for accuracy\n",
    "- Examples of these are:\n",
    "    1. **Reviewing tool calls**: Humans can review, edit or approve which tools to call by the agent.\n",
    "    1. **Validating `LLM` outputs**: Humans can review, edit or approve the outputs generated by `LLMs`.\n",
    "    1. **Providing context**: You can have the `agent` or `LLM` return control to the user. This means that the agent will pause and ask the human or user for validation before executing an action. (For example, asking for more information, or clarifying steps to take)\n",
    "\n",
    "### Prompting\n",
    "Prompting is a technique to control and provide instructions to how an `LLM` can process user inputs and generate responses. In `Agentic` systems, prompts are essential for:\n",
    "1. **Defining the agent's role or capabilities**: Your agent solution might have several agents working together. In that case, having separate instructions and guidance for agents can be provided through prompts.\n",
    "1. **Structuring the agent's thinking process**: Your agent's thinking process is defined by the instructions that you provide in the prompt. This includes if you want your agent to execute certain tools before the others for example.\n",
    "    \n",
    "**Note**: Prompt engineering is a separate topic and more information on this can be found [here](https://aws.amazon.com/what-is/prompt-engineering/). Prompt templates will be different from model-to-model to get the best performance for your agent use case.\n",
    "\n",
    "### State management\n",
    "State management in agents refer to tracking and updating the variables that the agent uses based on the conversation history, task progress, available resources and user preferences.\n",
    "\n",
    "## Agent frameworks\n",
    "There are several Agent frameworks that can help us build stateful, persistent agents. These frameworks are either open-source (Such as [LangGraph](https://www.langchain.com/langgraph), [Letta](https://www.letta.com/), [LangChain](https://www.langchain.com/), [CrewAI](https://www.crewai.com/)) or AWS-offered, such as [Agents on Amazon Bedrock](https://aws.amazon.com/bedrock/agents/). In this lab, we will be taking an example of `LangGraph` to build your simple agent with memory.\n",
    "\n",
    "# Build a simple agent using `LangGraph`\n",
    "\n",
    "In this lab, we will use `LangGraph` to build a simple agent. `LangGraph` is an open-source low-level orchestration framework for building controllable agents. This library enables agent orchestration, offering customizable architectures such as long-term memory, `HITL`, custom tools, etc. `LangGraph` provides low-level supporting infrastructure that sits underneath any workflow or agent. It does not abstract prompts or architecture, and provides three central benefits:\n",
    "\n",
    "#### Persistence\n",
    "`LangGraph` has a persistence layer, which offers a number of benefits:\n",
    "\n",
    "1. `Memory`: LangGraph persists arbitrary aspects of your application's state, supporting memory of conversations and other updates within and across user interactions;\n",
    "1. `Human-in-the-loop`: Because state is `checkpointed`, execution can be interrupted and resumed, allowing for decisions, validation, and corrections via human input.\n",
    "\n",
    "#### Streaming\n",
    "`LangGraph` also provides support for streaming workflow / agent state to the user (or developer) over the course of execution. `LangGraph` supports streaming of both events (such as feedback from a tool call) and tokens from LLM calls embedded in an application.\n",
    "\n",
    "#### Debugging and Deployment\n",
    "`LangGraph` provides an easy onramp for testing, debugging, and deploying applications via `LangGraph` Platform. This includes Studio, an IDE that enables visualization, interaction, and debugging of workflows or agents. This also includes numerous options for deployment.\n",
    "\n",
    "#### In this lab we will:\n",
    "\n",
    "1. Build a simple agent powered by an `LLM`.\n",
    "\n",
    "1. Create the `LangGraph` graph state which will have `nodes` as python functions and edges as components maintaining the flow between the nodes.\n",
    "\n",
    "1. Invoke the agent.\n",
    "\n",
    "1. Persist short term and long term memory within the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets import the necessary libraries required to build the agent in this notebook\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph components\n",
    "---\n",
    "\n",
    "`LangGraph`, at its core models agent workflows as graphs. These graphs contain nodes, edges and a state. LangGraph revolves around the concept of stateful graph, where each node in the graph represents a step in the application (could be an agent or an action) and the graph maintains a state that is passed around and updated as the computation progresses. You can define the behavior of your agent using these three components:\n",
    "\n",
    "1. `State`: The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "1. `StateGraph`: This is a shared data structure that represents the current snapshot of your graph of your application.\n",
    "\n",
    "1. `Nodes`: These are python functions that contain the logic to your agents. There are two main nodes:\n",
    "    - `Agent node`: An agent node is responsible for deciding on which actions to take.\n",
    "    - `Tool node`: The tool node will orchestrate calling a respective tool, performing some computation and then returning the output back to the user with the updated state.\n",
    "\n",
    "1. `Edges`: These control the flow between nodes. These are python functions that contain logic to determine which `Node` to call next based on the current `state`. This is a big part of how your agents work and how different nodes communicate with one another. There are a few concepts to edges:\n",
    "\n",
    "    - **Normal Edges**: Go directly from one node to the next.\n",
    "    - **Conditional Edges**: Call a function to determine which node(s) to go to next.\n",
    "    - **Entry Point**: Which node to call first when user input arrives.\n",
    "    - **Conditional Entry Point**: Call a function to determine which node(s) to call first when user input arrives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Agent State\n",
    "\n",
    "We will first define the agent state that will remain throughout the operation. This is the state of the graph and the state schema serves as the input for all nodes and edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    \"\"\"\n",
    "    This is the state of the planner agent. It contains the following fields:\n",
    "    - `messages`: The messages in the conversation.\n",
    "    - `itinerary`: The itinerary generated by the agent.\n",
    "    - `city`: The city for which the itinerary is generated.\n",
    "    - `user_message`: The user message containing the query.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], \"The messages in the conversation\"]\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up language models and prompts\n",
    "\n",
    "Next, we will set up LLM and a prompt template that will be used by our agent in planning the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the global variables used across this notebook\n",
    "BEDROCK_RUNTIME: str = 'bedrock-runtime'\n",
    "# Model ID used by the agent\n",
    "AMAZON_NOVA_LITE_MODEL_ID: str = \"us.amazon.nova-lite-v1:0\"\n",
    "PROVIDER_ID : str = 'amazon'\n",
    "# Inference parameters\n",
    "TEMPERATURE: float = 0.1\n",
    "MAX_TOKENS: int = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "# We are importing this to use any model supported on Amazon Bedrock. In this example\n",
    "# we will be using the Amazon Nova lite model.\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "# This helps checkpoint the memory state of the agent for short term/long term memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "logger.info(f\"Running this example in region: {region}\")\n",
    "\n",
    "# Initialize the bedrock client placeholder\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm used by the agent\n",
    "llm = ChatBedrockConverse(\n",
    "    model=AMAZON_NOVA_LITE_MODEL_ID,\n",
    "    provider=PROVIDER_ID, \n",
    "    temperature=TEMPERATURE, \n",
    "    max_tokens=MAX_TOKENS,\n",
    "    client=bedrock_client,\n",
    ")\n",
    "\n",
    "# Initialize the itinerary prompt that will be used across the agent workflow. This gives agent instructions and guidance while executing tasks based\n",
    "# on the user request. This prompt will be used by the LLM.\n",
    "\n",
    "# Note that this prompt contains placeholder variables {city} and {user_message} that will be replaced with the user input and city name respectively.\n",
    "ITINERARY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful travel assistant. Create a day trip itinerary for {city} based on the user's interests. \n",
    "    Follow these instructions:\n",
    "    1. Use the below chat conversation and the latest input from Human to get the user interests.\n",
    "    2. Always account for travel time and meal times - if its not possible to do everything, then say so.\n",
    "    3. If the user hasn't stated a time of year or season, assume summer season in {city} and state this assumption in your response.\n",
    "    4. If the user hasn't stated a travel budget, assume a reasonable dollar amount and state this assumption in your response.\n",
    "    5. Provide a brief, bulleted itinerary in chronological order with specific hours of day.\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{user_message}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the nodes and Edges\n",
    "---\n",
    "\n",
    "Next, we will be adding nodes, edges and persistent memory to the `StateGraph` before we compile it.\n",
    "\n",
    "1. user travel plans\n",
    "1. invoke with Bedrock\n",
    "1. generate the travel plan for the day\n",
    "1. ability to add or modify the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first node that we will create will solely input a user message\n",
    "def input_interest(state: PlannerState) -> PlannerState:\n",
    "    \"\"\"\n",
    "    This function takes a PlannerState object as input and prompts the user to enter their interest.\n",
    "    It then updates the PlannerState object with the user's interest and returns the updated object.\n",
    "    \"\"\"\n",
    "    # The user message is fetched from the state during invocation. The stateGraph is traversed during this computation\n",
    "    user_message = state['user_message'] # this comes from the user input\n",
    "    if not state.get('messages'): state['messages'] = []\n",
    "    return {\n",
    "        **state\n",
    "    }\n",
    "\n",
    "# The next node is responsible for creating an agent invocation based on the user interest\n",
    "def create_itinerary(state: PlannerState) -> PlannerState:\n",
    "    \"\"\"\n",
    "    This function takes a PlannerState object as input and creates an agent invocation based on the user's interest.\n",
    "    It then updates the PlannerState object with the agent invocation and returns the updated object.\n",
    "    \"\"\"\n",
    "    # Format the user input into the prompt, and any relevant message history. This contains the city, user message and any relevant chat history.\n",
    "    response = llm.invoke(ITINERARY_PROMPT.format_messages(city=state['city'], user_message=state['user_message'], chat_history=state['messages']))\n",
    "    print(\"\\nItinerary plan:\")\n",
    "    print(response.content)\n",
    "    return {\n",
    "        **state,\n",
    "        'messages': [\n",
    "            *state['messages'],\n",
    "            HumanMessage(content=state['user_message']),\n",
    "            AIMessage(content=response.content)\n",
    "        ],\n",
    "        'itinerary': response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Graph\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we will create our `LangGraph` workflow and then compile it.\n",
    "\n",
    "1. First, we will initialize the `StateGraph` with the `State` class that we defined above\n",
    "1. Then, we add our nodes and edges.\n",
    "1. We use the `START` Node, a special node that sends user input to the graph, to indicate where to start our graph.\n",
    "1. The `END` Node is a special node that represents a terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the StateGraph\n",
    "workflow = StateGraph(PlannerState)\n",
    "# Next, we will add our nodes to this workflow\n",
    "workflow.add_node(\"input_user_interests\", input_interest)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "workflow.set_entry_point(\"input_user_interests\")\n",
    "# Next, we will add a direct edge between input interests and create itinerary\n",
    "workflow.add_edge(\"input_user_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Memory?Â¶\n",
    "Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future.\n",
    "\n",
    "As AI agents undertake more complex tasks involving numerous user interactions, equipping them with memory becomes equally crucial for efficiency and user satisfaction. With memory, agents can learn from feedback and adapt to users' preferences. This guide covers two types of memory based on recall scope:\n",
    "\n",
    "1. `Short-term memory`, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. `LangGraph` manages short-term memory as a part of your agent's state. State is persisted to a database using a `checkpointer` so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\n",
    "\n",
    "1. `Long-term memory` is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. `LangGraph` provides stores (reference doc) to let you save and recall long-term memories.\n",
    "\n",
    "Both are important to understand and implement for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create a checkpointer which will let us have the graph persist its state\n",
    "# this is a complete memory for the entire graph\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have compiled the graph, we can view it in a mermaid diagram. You can use Mermaid\n",
    "# to generate diagrams from markdown-like text.\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that from the start point, the user can input the message or their interests for the itinerary. Next, according to the next node, the llm will be invoked with the custom prompt template to create that itinerary. This is a hello world example of a simple agent on `LangGraph`.\n",
    "\n",
    "### Define the function that runs the graph\n",
    "---\n",
    "\n",
    "When we compile the graph, we turn it into a `LangChain` Runnable, which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs. In the following example, we run `stream()` to invoke the graph with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, city_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    print(f\"City of interest: {city_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : city_request}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run the example\n",
    "\n",
    "# thread_id within the configurable object serves as a unique identifier for the conversation session. This ID is used by the graph to: Track which conversation a message belongs to and\n",
    "# maintain separate conversation histories for different users or sessions\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# I want to create an itinerary for a day trip in seattle with boaring and swimming options. Make it extremely comprehensive and should include meals as well\n",
    "user_request = input(\"Enter your travel request: \")\n",
    "# Seattle\n",
    "city = input(\"Enter the city: \")\n",
    "\n",
    "# Use them in the function call\n",
    "run_travel_planner(user_request, city, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage the memory saver to manipulate the Graph State\n",
    "\n",
    "1. Since the Conversation Messages are part of the graph state we can leverage that\n",
    "\n",
    "1. However the graph state is tied to session_id which will be passed in as a thread_id which ties to a session\n",
    "\n",
    "1. If we add a request with different thread id it will create a new session which will not have the previous Interests\n",
    "\n",
    "1. However this this has the other check points variables as well and so this pattern is good for A-Sync workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run the example\n",
    "\n",
    "# thread_id within the configurable object serves as a unique identifier for the conversation session. This ID is used by the graph to: Track which conversation a message belongs to and\n",
    "# maintain separate conversation histories for different users or sessions\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Can you add picnic to this itinerary?\n",
    "user_request = input(\"Enter your travel request: \")\n",
    "# Seattle\n",
    "city = input(\"Enter the city: \")\n",
    "\n",
    "# Use them in the function call\n",
    "run_travel_planner(user_request, city, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new section for picnic would have been added to the previous response since we used the same thread id for this call: \n",
    "\n",
    "```\n",
    "Picnic Lunch at Gas Works Park\n",
    "- **Location:** 1421 N. Lake Washington Blvd, Seattle, WA 98109\n",
    "- **Description:** Head to Gas Works Park for a picnic lunch. The park offers beautiful views of the Seattle skyline and Lake Union.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's run with another session and we should see a different response since that thread is not related to anything, \n",
    "# it will create a new itinerary with different items from the above 2 examples.\n",
    "config = {\"configurable\": {\"thread_id\": \"13\"}}\n",
    "\n",
    "# Can you add picnic to this itinerary?\n",
    "user_request = input(\"Enter your travel request: \")\n",
    "# Seattle\n",
    "city = input(\"Enter the city: \")\n",
    "\n",
    "# Use them in the function call\n",
    "run_travel_planner(user_request, city, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "Memory is key for any agentic conversation which is `Multi-Turn` or `Multi-Agent` collaboration conversation and more so if it spans multiple days. The 3 main aspects of Agents are:\n",
    "\n",
    "1. Tools\n",
    "1. Memory\n",
    "1. Planners\n",
    "\n",
    "### Types of Memory\n",
    "\n",
    "Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Consider the frustration of working with a colleague who forgets everything you tell them, requiring constant repetition! As AI agents undertake more complex tasks involving numerous user interactions, equipping them with memory becomes equally crucial for efficiency and user satisfaction. With memory, agents can learn from feedback and adapt to users' preferences. This guide covers two types of memory based on recall scope:\n",
    "\n",
    "1. Short-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. LangGraph manages short-term memory as a part of your agent's state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\n",
    "\n",
    "1. Long-term memory is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores to let you save and recall long-term memories.\n",
    "\n",
    "In this example lab, we will be leveraging a `multi-thread`, `multi-session` persistence to chat messages. That way, we will be able to retrieve memory across various sessions and chats across several users. Ideally you would leverage persistence like Redis store to save messages per session.\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "1. We can have several Patterns - we can have each Agents with it's own Session memory\n",
    "1. Or we can have the whole Graph have a combined memory in which case each agent will get it's own memory\n",
    "\n",
    "The `MemorySaver` or the Store have the concept of separating sections of memory by Namespaces or by Thread ID's and those can be leveraged to either 1/ Use the graph level message or memory 2/ Each agent can have it's own memory via space in saver or else having it's own saver like we do in the `ReACT` agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore, Item, Op, Result\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Any, Iterable, Literal, NamedTuple, Optional, Union, cast\n",
    "\n",
    "# CustomMemoryStore is a wrapper class that implements the BaseStore interface\n",
    "class CustomMemoryStore(BaseStore):\n",
    "    def __init__(self, ext_store):\n",
    "        # Initialize with an external store that will handle the actual storage\n",
    "        self.store = ext_store\n",
    "\n",
    "    def get(self, namespace: tuple[str, ...], key: str) -> Optional[Item]:\n",
    "        # Retrieve an item from the store using namespace and key\n",
    "        return self.store.get(namespace, key)\n",
    "\n",
    "    def put(self, namespace: tuple[str, ...], key: str, value: dict[str, Any]) -> None:\n",
    "        # Store a value in the store using namespace and key\n",
    "        return self.store.put(namespace, key, value)\n",
    "        \n",
    "    def batch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        # Execute multiple operations in a batch\n",
    "        return self.store.batch(ops)\n",
    "        \n",
    "    async def abatch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        # Execute multiple operations asynchronously in a batch\n",
    "        return self.store.abatch(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = CustomMemoryStore(InMemoryStore())\n",
    "namespace_u = (\"chat_messages\", \"user_id_1\")\n",
    "key_u=\"user_id_1\"\n",
    "in_memory_store.put(namespace_u, key_u, {\"data\":[\"list a\"]})\n",
    "item_u = in_memory_store.get(namespace_u, key_u)\n",
    "print(item_u.value, item_u.value['data'])\n",
    "\n",
    "in_memory_store.list_namespaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the similiar graph as earlier -- note we will not have any mesages in the Graph state as that has been externalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_interests(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    user_message = state['user_message'] #input(\"Your input: \")\n",
    "    return {\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    #- get the history from the store\n",
    "    user_u = f\"user_id_{config['configurable']['thread_id']}\"\n",
    "    namespace_u = (\"chat_messages\", user_u)\n",
    "    store_item = store.get(namespace=namespace_u, key=user_u)\n",
    "    chat_history_messages = store_item.value['data'] if store_item else []\n",
    "    print(user_u,chat_history_messages)\n",
    "\n",
    "    response = llm.invoke(ITINERARY_PROMPT.format_messages(city=state['city'], user_message=state['user_message'], chat_history=chat_history_messages))\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "\n",
    "    #- add back to the store\n",
    "    store.put(namespace=namespace_u, key=user_u, value={\"data\":chat_history_messages+[HumanMessage(content=state['user_message']),AIMessage(content=response.content)]})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"itinerary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store_n = CustomMemoryStore(InMemoryStore())\n",
    "\n",
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "\n",
    "app = workflow.compile(store=in_memory_store_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : \"London\"}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "user_request = \"Can you create a itinerary for a day trip in london.  I need a complete plan that budgets for travel time and meal time.\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "user_request = \"Can you add something else to this?\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick look at the store\n",
    "it will show the History of the Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_memory_store_n.list_namespaces())\n",
    "print(in_memory_store_n.get(('chat_messages', 'user_id_2'),'user_id_2').value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
